[parameters]

# --- PROGRAM FLOW --- #
human_mode = False
experiments = False

# --- EXPERIMENT PARAMETERS --- #
actor_learning_rates = [0.2, 0.7, 0.8]
critic_learning_rates = [0.2, 0.7]
decay_discount_values = [0.8, 0.9]
win_multipliers = [100]
initial_epsilons = [0.8]
exploitation_thresholds = [0, 1]
iterations = 50


# --- ENVIRONMENT CONFIGURATION --- #
action_reward = 0
win_multiplier = 100
reinforcement = 1

# -- TASK 2: 5x5 triangle -- #
board_type = triangle
board_size = 5
empty_nodes = [(2, 1)]

# -- TASK 3: 4x4 diamond -- #
; board_type = diamond
; board_size = 4
; empty_nodes = [(3,1)]


# --- LEARNING CONFIGURATION --- #
episodes = 600
test_episodes = 10
nn_dimentions = [50]
nn_activation_functions = ['relu']

# -- NN Critic -- #
; nn_critic = True
; critic_learning_rate = 0.2
; actor_learning_rate = 0.8
; critic_decay_rate = 0.8
; actor_decay_rate = 0.9
; critic_discount_factor = 0.8
; actor_discount_factor = 0.9

# -- Table Critic -- #
nn_critic = False
critic_learning_rate = 0.7
actor_learning_rate = 0.8
critic_decay_rate = 0.9
actor_decay_rate = 0.9
critic_discount_factor = 0.9
actor_discount_factor = 0.9


# --- EPSILON CONFIGURATION --- #
epsilon = 0.8
epsilon_decay = 0.985

# -- TASK 2: 5x5 triangle -- #
linear_epsilon = False
exploitation_threshold = 1

# -- TASK 3: 4x4 diamond -- #
; linear_epsilon = True
; exploitation_threshold = 0



# --- VISUALIZATION CONFIGURATION --- #
visualize_without_convergence = True
visualization_delay = 0.5
